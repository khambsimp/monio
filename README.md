# monios 0.0.1
The fully functional Operating System.

A refreshed OS with a different approach to utilize Theory of Computing
to develop a blueprint of a Computer Architecture that is beneficial to the society of the future.

The way that Software and Computer development is done does not actually take into consideration how people have developed and grown their understanding of Computing in particular, or Information in general. A common misconception might be our tendency to expect current conceptualizations of Computing to be lead by the needs of Scientists, when many of the greatest applications of Computing may be initiated from people outside of Science. This is a conceptualization of Computing as a concept more broad and able to impact society at a greater magnitude.

# Language
We begin our study of Computing with an interrogation. The way Language is manifested within our Cognition is the same between people. Is it not strange, that when we develop literature we would interrogate the English lexicon, oftentimes pulling from phrase, or parable? In a similar way when we develop architecture, or develop communication in Binary form we interrogate our systematic framework of encoded language. For this reason generative grammar, Vedic grammar, and Goedel's parable all reference a similar interrogation. Language is structured, but in the same way decrepit. All Language is actually the same language.

The independent development of written language has occurred only four times in recorded history. And as such we our hesitant to develop new Symbolic Languages unless forced to by societal factors. In a similar way it is often societal factors which initiate our retreat to our so basic of instincts with the desire to start again. Therefore it is amazing that information has brought us to examine this underpinning of understanding again.

# Personal Computing
There are similarities in the way that human beings comprehend Computers. In essence it amounts to how we architect information in a way that it can be processed correctly, and computably using logic, arithmetic, and integer math. To process large swathes of information we tend to develop abstractions. These abstractions can be deduced to amount to logic, arithmetic, and integers when analyzed and optimized. However, we use abstractions to communicate with other humans, based on the specific way the computable functions have been implemented in physical reality. This project address how, our theoretical, and mathematical conceptualizations of computing affect, the implemented and as designed languages, and processors used to store and compute the information within our lives.

# Cloud Computing
A great resource on this end is the aptly name Warehouse Scale Computer. Computing has been framed in terms of size (volume), weight (mass), and power (energy) as quantitative design elements for an efficient computing platform. Lo and behold these tradeoffs are not unique to vehicles themselves. On the contrary, Computing and its operations at scale are often framed in reference to time, or energy (resource) efficiency. In other words, as people build larger and larger computing platforms, we also find that large scale building renditions of these must also operate efficiently in order to justify the resources used to develop them. In other words Computing paradigms are not only important for their impact, and reframing of information, but also their efficiency. How do operations and energy reformat and codify large amounts of information and data? Is this reformatting effective to society? Universally available?

# Distributed Computing
The goal of Monoid Internet is to explore what an information minicomputer could be. The question at hand is, is there a better way to design computers than the method we are using right now? The answer is probably yes. Computers are information manipulators at their core, so programming can really look however we would like it to. It more so depends on the outcome you're looking for.

When developing an architecture to describe how computers can maintain network synchronized information, it's important to develop a framework, which can
span from two individual computers to many. An information format which both encodes network properties, and transposes oncoming data. This is similar to JSON binary format. One could imagine a scenario were the Network encoding defined for transmission across a Wired, or Wireless Networking protocol, Global Network Transmission would seem near real time, including in the scenario of a Database.

The information computer is also vital for determining how to codify, and distribute algorithmic encoders which can develop information architecture efficiently. For instance Git is a form of radio, it is capable of sitting as an https endpoint on a host computer, and store the eventual commits of thousands of developers asynchronously. This enables programmers to develop software architecture concurrently, in a parallel fashion, and asynchronously.  

# A Computer is a Mathematical Construct
## Logic
Human Beings can reason about Computers *exactly*. Human Beings can develop written symbolic languages which represent the state of a Computer exactly. There are particular definitions of a set that represent all Discrete formulations of a Computing problem. Computers (Boolean Operators) and (Turing Complete Languages). Now these logical preceptors are also computability operators, and information encoders in the language argument and framework sense.
## Discrete Mathematics
Discrete Mathematics is a logical, quantitative basis for scalable Algorithms and Computer Science in general. Programmers require a foundational framework to reason about Computers, Data, Information, and Networks. Discrete Mathematics tends to focus on, and provide useful techniques for scalable constructs useful within a context of Computing. Programmers can program natural numbers and integers reflexively.
## Algorithms
Or Algebra designated for Discrete Mathematics, abstracts different approaches to define discrete functions using variables. Algebra and Algorithms are intimately related. Algorithms and Algorithmic Analysis institute a formal way to estimate the performance of a program using analytic methods. algorithmic Analysis is ever more important. Similar to Real Analysis, Algorithmic Analysis presents a number of techniques from Applied Complexity Theory, Applied Programming Language Theory, and Applied Automata theory to derive and compare the real world performance of Algorithms and Data Structures. To what extent can we determine the expected as implemented program to run in finite time or energy. To what extent do these match performance of engineered Computers?
## Data Structures
Oftentimes to define efficient algorithms a symbolic representation must be created for both the algorithm and the data it operates upon.  When Algorithmic Analysis is applied to large scale programs, the problem of how memory and data are accessed impact the performance of a program and its scalable nature.
## Cryptography
Cryptography includes the preservation of information across physical spaces and in the event that data has been corrupted intentionally, this is especially important in proving the validity of data within networks.
## Programming Language Theory
Programming Language theory includes the development of natural language shorthand that is related Turing machines which operate on discrete systems in a way which people can reason about them.
## Type Theory
Type Theory is a Logic  Language created to further abstract the consolidation of language within a programming language, making complex functions succint in their description of off nominal notions.
## Formal Languages
Formal Languages are logic languages more related to shorthand logic and discrete notions with a natural language.
## Theory of Computation
Theory of Computation is large collection of organized methods for using number theory to describe the construction and description of large computable and realizable systems that can be reasoned about to the same magnitude. Theory of Computation derrides a few fundamental changes in aspects and approaches for the development of computing resources. These include the Theory of Cmputations, Computability Theory, Automata Theory and Complexity Theory. Theory of Computation encompasses our methods of making computing, simpler, more time and energy efficient, and easier for programmers to express.  
## Computability Theory
This concerns where and to what extent ever more complex computable languages can be written. One could imagine a case where, even though, a particular abstract argument about computing large scale data were made, the impediment to actually simulate an appropriate calculation would be the framework, or basis language of the computation. That is, when given a set of computable rules, a Computer can distill an argument in abstract human terms. The language and whether the recursive process is definable within the computer language makes computability a conscious abstract solution to a posed data analysis. If it is impossible to define a computing language that can distill whether any mathematical statement were true or false, what is the minimum number of rules and guidance that should be made so that a computer or definable language could determine the extent to which a  mathematical statement is correct or false?
## Complexity Theory
In essence Computing is an excersize in managing Complexity. As Software projects grow, so do the number of referenced helper libraries, and related extractions used to limit the processing involved to provide a usable software definition. As the number of dependencies grow for sophisticated software frameworks, so does the possibility that a dependent element may break or be infected by malware. Languages must be both expressive, yet also maintain their operating intent regardless of how the algorithm is framed. Encoding functions must conversely not be described by equally complex and dependent software entities. The Art of managing Complexity, Systems Theory, and Architecture are all exercises in managing and scaling Simplicity. In a real sense when Computers do not scale, Complexity has masked the underlying intent of the Program. Famous problems include P=NP and this theory is linked directly to the field of Cryptography.
## Graph Theory
Graph theory is closely related to Network Theory and the development of nodes with an emphasis of how they related to one another instead of what their value is individually. Graphs are useful for modeling dependent relationships between a single entity and its m=peers. For Peer to Peer Computing, Graphs may relate the interdependent relationships between data members. For protocol engineering the network itself may track relationships between entities, however this may be information which can be tracked only within the near term.
## Automata Theory
Automata Theory and Turing Machines are class of entities utilized by Computer Scientists, and Formal Language Theorists to reason about the performance of Algorithms. That is when algorithms are applied, there are aspects of their behavior which are limited in time, and or memory, that Automata are able to model in Discrete form. Notable as well is that
## Information Theory
At its core the study of Information concerns Human Beings, and the source of their Language capabilities. Human Beings have developed Language over time to to develop written, auditory, handsignal, and other renditions of language which can be percieved by their senses. Lingustics, natural language, math, are all attemps by people to codify and extend their senses and consitiutue ad physical rendition of information. An information which does not only rever to Natural Phenomena but, may refer to aspects of human creativity, history, folklore, and abstract. Information Theory constitutes the until now understanding of how information can be perserved and communicated to human beings, and the extent to which we can construe and perserve language over time,
and communicate it with precision.
## Coding Theory
One of the great efficiencies in Computer Science is being able to say a lot with a little. That is not to just optimize processing, but to determine what and why information in particular should be processed in the first place. In a sense Computer Scientists analyze their framing of a problem in the same way they are adept at picking out to to use when working on a particular one. The Art of Programming is an exercise in Coding Theory. We aim to take information presented, and from it generate an equivalent representation. How that encoding is done is the Art.
## Category Theory
Category Theory is a method, for categorizing relationships between different field and implementations of math. Fields of Math which seem different often have known methods for converting or applying constituent objects with one another. Category theory provides a set of common techniques to do so, while simplifying, and making more understandable, applications of proofs using different kinds of mathematics. Part Systems language, and part programming language for mathematics. Category Theory is considered an approach of mathematics which can serve as a common language for math.
## Group Theory
Group Theory, similar to category theory is thought of as a possible common language for mathematics, and involves the unifying of fields which seem dissimilar. Groups constitute different objects within mathematics, which can serve as a data structure to organize common objects utilized to define mathematical fields, and their interaction.
## Combinatorics
## Machine Learning
There are a number of techniques for conducting Coding Theory, that is utilizing discrete data to develop codified information of data within multiple layers of abstraction.
## Artifical Intelligence
Artificial Intelligence is form of Digital Filtering involving parallel architectures. These are often described as matrices, however, these are topologically similar to neural nets. These matrices define architectures for storing associations between statistically consistent data, one of these being written speech.
# Blog
This is the first instantiation of the monio computer, an information translator. Input is markdown, and this is graphically communicated to the observer through repeated exposure. The direct approach.
# Computing Paradigms
## Distributed Computing
## Cloud Computing
## Personal Computing
## Internet of Things
## Graphic Design System 3
The new Graphic Design System 3 is developed using Non-uniform Rational B-Splines.
























.
