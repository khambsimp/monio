# monios 0.0.1
The fully functional Operating System.

A refreshed OS with a different approach to utilize Theory of Computing
to develop a blueprint of a Computer Architecture that is beneficial to the society of the future.

The way that Software and Computer development is done does not actually take into consideration how people have developed and grown their understanding of Computing in particular, or Information in general. A common misconception might be our tendency to expect current conceptualizations of Computing to be lead by the needs of Scientists, when many of the greatest applications of Computing may be initiated from people outside of Science. This is a conceptualization of Computing as a concept more broad and able to impact society at a greater magnitude.

# Language
We begin our study of Computing with an interrogation. Is the way Language is manifested within our Cognition is the same between people? *the way that language tends to develop abstract concepts is similar enough that we should question why* Is it not strange, that when we develop literature we would interrogate the English lexicon, oftentimes pulling from phrase, or parable? In a similar way when we develop architecture, or develop communication in Binary form we interrogate our systematic framework of encoded language. For this reason generative grammar, Vedic grammar, and Goedel's parable all reference a similar interrogation. Language is structured, but in the same way decrepit. All Language is actually the same language.

The independent development of written language has occurred only four times in recorded history. And as such we our hesitant to develop new Symbolic Languages unless forced to by societal factors. In a similar way it is often societal factors which initiate our retreat to our so basic of instincts with the desire to start again. Therefore it is amazing that information has brought us to examine this underpinning of understanding again. Within the realm of linguistics, and more specifically written, symbolic language, we find that to inter certain facilities within language, and to promote the development of intertwined systems whcih are persistent the development of Symbolic logics are pertinent. Algebra and Algorithms, the mosr familiar of these have been studied for millinea, however, as the systems which people develop become more interconnected, so does our use of symbolism, and our implementation of natural language grow, to develop arguments that can persist over time. When we seek a language to discuss magnitudes, and repeated behavior people tend to utilize decidable, pattern induced languages that are related to repreated symbols. The language elements which maintains and determines their decidability are also elements of study.

# Computing Paradigms
## Personal Computing
There are similarities in the way that human beings comprehend Computers. In essence it amounts to how we architect information in a way that it can be processed correctly, and computably using logic, arithmetic, and integer math. To process large swathes of information we tend to develop abstractions. These abstractions can be deduced to amount to logic, arithmetic, and integers when analyzed and optimized. However, we use abstractions to communicate with other humans, based on the specific way the computable functions have been implemented in physical reality. This project address how, our theoretical, and mathematical conceptualizations of computing affect, the implemented and as designed languages, and processors used to store and compute the information within our lives. Games. To an extent the gamification of programming does respect an aspect of how information and onowledge fundamentallu accumulage within society. By this is intentioned our tendencies to develop information like legos. Fundamentally what makes computability and recursion so alluring is the idea it 
wiuld enable us to develop a systematic information framework, thag oike loading a sonic game on gamecube, would first load this history and language framework we were familiar with, in the case Nintendo, and then within that framework be amble to navigate within an existing framework of a menu and levels, and where we would have a simple instrument or interface, be it a gamecube controller, or in this case a keyboard and command line interface. Either eay for people to have a small universe they can make isolqted changes within, this is key, small insulatd changes that qlthough deliberate lead to an outcome we can measure is immense. it feels like sonic superpowers. Also text generation on these devices is strqight up horrible, but will be remedied in time. In any case, being able to make changes builds confidence in a person to consistently make changes to a worldwide database over time slowly steering the accumulation of information itself. It is important to preserve this interface, such that people everywhere are able to whift and impact the development of society, in their own way
and at their  own pace. The cursor placement is literally illogical. 

## Cloud Computing
A great resource on this end is the aptly name Warehouse Scale Computer. Computing has been framed in terms of size (volume), weight (mass), and power (energy) as quantitative design elements for an efficient computing platform. Lo and behold these tradeoffs are not unique to vehicles themselves. On the contrary, Computing and its operations at scale are often framed in reference to time, or energy (resource) efficiency. In other words, as people build larger and larger computing platforms, we also find that large scale building renditions of these must also operate efficiently in order to justify the resources used to develop them. In other words Computing paradigms are not only important for their impact, and reframing of information, but also their efficiency. How do operations and energy reformat and codify large amounts of information and data? Is this reformatting effective to society? Universally available?

## Distributed Computing
The goal of Monoid Internet is to explore what an information minicomputer could be. The question at hand is, is there a better way to design computers than the method we are using right now? The answer is probably yes. Computers are information manipulators at their core, so programming can really look however we would like it to. It more so depends on the outcome you're looking for.

When developing an architecture to describe how computers can maintain network synchronized information, it's important to develop a framework, which can
span from two individual computers to many. An information format which both encodes network properties, and transposes oncoming data. This is similar to JSON binary format. One could imagine a scenario were the Network encoding defined for transmission across a Wired, or Wireless Networking protocol, Global Network Transmission would seem near real time, including in the scenario of a Database.

The information computer is also vital for determining how to codify, and distribute algorithmic encoders which can develop information architecture efficiently. For instance Git is a form of radio, it is capable of sitting as an https endpoint on a host computer, and store the eventual commits of thousands of developers asynchronously. This enables programmers to develop software architecture concurrently, in a parallel fashion, and asynchronously.

# A Computer is a Mathematical Construct
## Logic
Human Beings can reason about Computers *exactly*. Human Beings can develop written symbolic languages which represent the state of a Computer exactly. There are particular definitions of a set that represent all Discrete formulations of a Computing problem. Computers (Boolean Operators) and (Turing Complete Languages). Now these logical preceptors are also computability operators, and information encoders in the language argument and framework sense. What does a language which can be proved actually gain you? Does it mean you can use less symbols and definitions, once the axioms of said language are defined, does it mean that there are provable statements which are not apparent within the language that would be provable otherwise? Let's say, types, sets, and proofs are the logic language equivalents to a transitor, or to a single neuron. In this viewpoint, artificial automata, and biological automata, and written syntactic automata could model systems of comparable complexity. One of the best ways for people to manage large scale complexity is to develop symbolic logic that can describe or model interconnected systems, and then relate that to natural language.

## Discrete Mathematics
Discrete Mathematics is a logical, quantitative basis for scalable Algorithms and Computer Science in general. Programmers require a foundational framework to reason about Computers, Data, Information, and Networks. Discrete Mathematics tends to focus on, and provide useful techniques for scalable constructs useful within a context of Computing. Programmers can program natural numbers and integers reflexively.
## Algorithms
Or Algebra designated for Discrete Mathematics, abstracts different approaches to define discrete functions using variables. Algebra and Algorithms are intimately related. Algorithms and Algorithmic Analysis institute a formal way to estimate the performance of a program using analytic methods. algorithmic Analysis is ever more important. Similar to Real Analysis, Algorithmic Analysis presents a number of techniques from Applied Complexity Theory, Applied Programming Language Theory, and Applied Automata theory to derive and compare the real world performance of Algorithms and Data Structures. To what extent can we determine the expected as implemented program to run in finite time or energy. To what extent do these match performance of engineered Computers?
## Data Structures
Oftentimes to define efficient algorithms a symbolic representation must be created for both the algorithm and the data it operates upon.  When Algorithmic Analysis is applied to large scale programs, the problem of how memory and data are accessed impact the performance of a program and its scalable nature.
## Cryptography
Cryptography includes the preservation of information across physical spaces and in the event that data has been corrupted intentionally, this is especially important in proving the validity of data within networks.
## Programming Language Theory
Programming Language theory includes the development of natural language shorthand that is related Turing machines which operate on discrete systems in a way which people can reason about them.
## Type Theory
Type Theory is a Logic  Language created to further abstract the consolidation of language within a programming language, making complex functions succint in their description of off nominal notions. The type theory referenced here is more directly applicable to Martin Lowe Type Theory or Intuitionistic Type Theory, and is applied in Discrete Mathematic forum by way of Homotopy Type Theory. Homotopy Type Theory has found its way into Theorom Solvers an Program Type where it has been successful informalizing notions of proof based on Conjecture, and Hypothesis. This is 
## Formal Languages
Formal Languages are logic languages more related to shorthand logic and discrete notions with a natural language.
## Theory of Computation
Theory of Computation is large collection of organized methods for using number theory to describe the construction and description of large computable and realizable systems that can be reasoned about to the same magnitude. Theory of Computation derrides a few fundamental changes in aspects and approaches for the development of computing resources. These include the Theory of Cmputations, Computability Theory, Automata Theory and Complexity Theory. Theory of Computation encompasses our methods of making computing, simpler, more time and energy efficient, and easier for programmers to express.
## Computability Theory
This concerns where and to what extent ever more complex computable languages can be written. One could imagine a case where, even though, a particular abstract argument about computing large scale data were made, the impediment to actually simulate an appropriate calculation would be the framework, or basis language of the computation. That is, when given a set of computable rules, a Computer can distill an argument in abstract human terms. The language and whether the recursive process is definable within the computer language makes computability a conscious abstract solution to a posed data analysis. If it is impossible to define a computing language that can distill whether any mathematical statement were true or false, what is the minimum number of rules and guidance that should be made so that a computer or definable language could determine the extent to which a  mathematical statement is correct or false?
## Complexity Theory
In essence Computing is an excersize in managing Complexity. As Software projects grow, so do the number of referenced helper libraries, and related extractions used to limit the processing involved to provide a usable software definition. As the number of dependencies grow for sophisticated software frameworks, so does the possibility that a dependent element may break or be infected by malware. Languages must be both expressive, yet also maintain their operating intent regardless of how the algorithm is framed. Encoding functions must conversely not be described by equally complex and dependent software entities. The Art of managing Complexity, Systems Theory, and Architecture are all exercises in managing and scaling Simplicity. In a real sense when Computers do not scale, Complexity has masked the underlying intent of the Program. Famous problems include P=NP and this theory is linked directly to the field of Cryptography.
## Systems Theory
Systems Theory constitutes the techniques people use to manage Complexity within real world applied systems. At the same time it constitutes the difficult in arranging multiple involved disciplines, and managing interactions that are not logically definable in an immediate sense, think the Halting Problem within Computability Theory. Systems is a method of defining a workflow, to provide the best possible chance to develop within the limit of a "Halting Problem" whilst still being able to nogotiate between a usable system, and one which is not. One of the most predominant problems in Computer development today is developing an environment which is binary compatible between different general purpose computing instatiations. The second, is once a similar environment has been defininded how are changes at different computing instantiations made independently such that improvements in oreder of magnitude, or in order of multitude are possbilt to conserve the overall systems net energy usage, timing, or sumber of elements required symbolically to preserve its complex performance? Were Computing a design and architecture problem, then presumably by developing the optimum way to express a design such that the most efficient performance is achievable would consitute a Science where by quatification would suggest the most efficient designs possible given basic axioms are arguments about the development of the consittuent Automatons.
## Graph Theory
Graph theory is closely related to Network Theory and the development of nodes with an emphasis of how they related to one another instead of what their value is individually. Graphs are useful for modeling dependent relationships between a single entity and its m=peers. For Peer to Peer Computing, Graphs may relate the interdependent relationships between data members. For protocol engineering the network itself may track relationships between entities, however this may be information which can be tracked only within the near term.
## Automata Theory
Automata Theory and Turing Machines are class of entities utilized by Computer Scientists, and Formal Language Theorists to reason about the performance of Algorithms. That is when algorithms are applied, there are aspects of their behavior which are limited in time, and or memory, that Automata are able to model in Discrete form. Notable as well is that, the way that sciety seems to be developing would suggest that Automata are becoming more a framework for building and developing Computers that Computers themselves. In a world where Automata are prevalent, it becomes more prfound action to prgram them and issue them in a way which is decisive.
## Cybernetics
Cybernetics more colesely involves the interplay between the human and the machine. The extent to which bilogical entities en masse may adhere to or organize themselves in mechanistic ways to be efficient as a system. This is important within the context of Automata, because, Automata are regulat in their construction, and in the way that they interpret language. To be able to coordinate many Automata also invovles a certain understaing of both language and they way that Complex Systems tend to evolve, especially in their tendancy to reach a state of survival.
## Information Theory
At its core the study of Information concerns Human Beings, and the source of their Language capabilities. Human Beings have developed Language over time to to develop written, auditory, handsignal, and other renditions of language which can be percieved by their senses. Lingustics, natural language, math, are all attemps by people to codify and extend their senses and consitiutue ad physical rendition of information. An information which does not only rever to Natural Phenomena but, may refer to aspects of human creativity, history, folklore, and abstract. Information Theory constitutes the until now understanding of how information can be perserved and communicated to human beings, and the extent to which we can construe and perserve language over time,
and communicate it with precision.
## Coding Theory
One of the great efficiencies in Computer Science is being able to say a lot with a little. That is not to just optimize processing, but to determine what and why information in particular should be processed in the first place. In a sense Computer Scientists analyze their framing of a problem in the same way they are adept at picking out to to use when working on a particular one. The Art of Programming is an exercise in Coding Theory. We aim to take information presented, and from it generate an equivalent representation. How that encoding is done is the Art.
## Communication Theory
Communication Theory includes methods or techniques to derive ways to encode language systems within one another. A great example is in terms of definifng integers, it is known that integers can essentially be encoded or written in terms of any other integer greater than intself. However, utilizing concepts of promes or of even and odd numbers it becomes more worthwhile to encode certain integers in terms of one another using mathematical properties of integers to decrease the amount of information or bits required to define the number itself.
## Category Theory
Category Theory is a method, for categorizing relationships between different field and implementations of math. Fields of Math which seem different often have known methods for converting or applying constituent objects with one another. Category theory provides a set of common techniques to do so, while simplifying, and making more understandable, applications of proofs using different kinds of mathematics. Part Systems language, and part programming language for mathematics. Category Theory is considered an approach of mathematics which can serve as a common language for math.
## Group Theory
Group Theory, similar to category theory is thought of as a possible common language for mathematics, and involves the unifying of fields which seem dissimilar. Groups constitute different objects within mathematics, which can serve as a data structure to organize common objects utilized to define mathematical fields, and their interaction.
## Combinatorics
## Machine Learning
There are a number of techniques for conducting Coding Theory, that is utilizing discrete data to develop codified information of data within multiple layers of abstraction.
## Artifical Intelligence
Artificial Intelligence is form of Digital Filtering involving parallel architectures. These are often described as matrices, however, these are topologically similar to neural nets. These matrices define architectures for storing associations between statistically consistent data, one of these being written speech.
# Blog
This is the first instantiation of the monio computer, an information translator. Input is markdown, and this is graphically communicated to the observer through repeated exposure. The direct approach.
## Internet of Things
## Graphic Design System 3
The new Graphic Design System 3 is developed using Non-uniform Rational B-Splines.
























.
